{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971c727",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import shap\n",
    "\n",
    "# Local imports from your project\n",
    "from algorithms.common import PolicyNetwork, DEFAULT_DEVICE, ACTION_DIM, STATE_DIM\n",
    "\n",
    "def load_policy(policy_path=\"Calum_Testing/ddpg_batch.pth\"):\n",
    "    \"\"\"Load a trained policy from a checkpoint file.\"\"\"\n",
    "    policy = PolicyNetwork().to(DEFAULT_DEVICE)\n",
    "    policy.load_state_dict(torch.load(policy_path, map_location=DEFAULT_DEVICE))\n",
    "    policy.eval()\n",
    "    print(f\"Loaded policy from {policy_path}. Expected input dim: {policy.fc1.in_features}\")\n",
    "    return policy\n",
    "\n",
    "def init_env():\n",
    "    \"\"\"Initialize the Walker2d environment (v5) with rendered RGB arrays.\"\"\"\n",
    "    env = gym.make(\"Walker2d-v5\", render_mode=\"rgb_array\")\n",
    "    return env\n",
    "\n",
    "def collect_states_random(env, num_samples=500):\n",
    "    \"\"\"Collect states by executing random actions.\"\"\"\n",
    "    states = []\n",
    "    observation, _ = env.reset()\n",
    "    for _ in range(num_samples):\n",
    "        states.append(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, _, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            observation, _ = env.reset()\n",
    "    return np.array(states)\n",
    "\n",
    "def collect_states_on_policy(env, policy, num_samples=500):\n",
    "    \"\"\"Collect states from running the trained policy (on-policy).\"\"\"\n",
    "    states = []\n",
    "    observation, _ = env.reset()\n",
    "    for _ in range(num_samples):\n",
    "        states.append(observation)\n",
    "        obs_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0).to(DEFAULT_DEVICE)\n",
    "        with torch.no_grad():\n",
    "            action = policy(obs_tensor).cpu().numpy()[0]\n",
    "        observation, _, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            observation, _ = env.reset()\n",
    "    return np.array(states)\n",
    "\n",
    "def wrapped_policy(x, policy):\n",
    "    \"\"\"\n",
    "    A wrapper function that converts numpy inputs to torch tensors, feeds them through the policy,\n",
    "    and converts the outputs back to a numpy array.\n",
    "    \"\"\"\n",
    "    # x is a numpy array of shape (n_samples, STATE_DIM)\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32).to(DEFAULT_DEVICE)\n",
    "    with torch.no_grad():\n",
    "        # The policy outputs a tensor of shape (n_samples, ACTION_DIM)\n",
    "        out = policy(x_tensor)\n",
    "    return out.detach().cpu().numpy()\n",
    "\n",
    "def run_shap_analysis(policy, env, bg_data_collection=\"on_policy\",\n",
    "                      num_bg_samples=200, num_explain_samples=50):\n",
    "    # Collect background data\n",
    "    if bg_data_collection == \"on_policy\":\n",
    "        print(\"Collecting on-policy background data...\")\n",
    "        background_data = collect_states_on_policy(env, policy, num_samples=num_bg_samples)\n",
    "    else:\n",
    "        print(\"Collecting random background data...\")\n",
    "        background_data = collect_states_random(env, num_samples=num_bg_samples)\n",
    "        \n",
    "    print(\"Background data shape (raw):\", background_data.shape)\n",
    "    \n",
    "    # Summarize the background data using kmeans to reduce runtime.\n",
    "    print(\"Summarizing background data using kmeans...\")\n",
    "    background_summary = shap.kmeans(background_data, 50)\n",
    "    # Access the underlying numpy array from the DenseData object:\n",
    "    background_summary_array = background_summary.data\n",
    "    print(\"Background summary shape:\", background_summary_array.shape)\n",
    "    \n",
    "    # Create a wrapped version of the policy function so that inputs are properly converted.\n",
    "    wrapped_fn = lambda x: wrapped_policy(x, policy)\n",
    "    \n",
    "    # Use the unified SHAP API by passing the wrapped function and the summarized background.\n",
    "    print(\"Creating unified SHAP explainer...\")\n",
    "    explainer = shap.Explainer(wrapped_fn, background_summary_array)\n",
    "    \n",
    "    # Collect states to explain (using on-policy data).\n",
    "    states_to_explain = collect_states_on_policy(env, policy, num_samples=num_explain_samples)\n",
    "    print(\"States to explain shape:\", states_to_explain.shape)\n",
    "    \n",
    "    # Compute SHAP values for the states.\n",
    "    print(\"Computing SHAP values...\")\n",
    "    shap_values = explainer(states_to_explain)\n",
    "    # For a vector-valued model with ACTION_DIM outputs, shap_values.values should have shape:\n",
    "    #   (num_explain_samples, STATE_DIM, ACTION_DIM)\n",
    "    print(\"SHAP values computed. Their shape is:\", shap_values.values.shape)\n",
    "    \n",
    "    # Define feature names for the 17-dimensional observation.\n",
    "    if STATE_DIM == 17:\n",
    "        feature_names = [\n",
    "            \"torso_height\", \"torso_angle\", \"left_thigh_angle\", \"left_knee_angle\", \"left_ankle_angle\",\n",
    "            \"right_thigh_angle\", \"right_knee_angle\", \"right_ankle_angle\", \"left_thigh_velocity\", \n",
    "            \"left_knee_velocity\", \"left_ankle_velocity\", \"right_thigh_velocity\", \"right_knee_velocity\",\n",
    "            \"right_ankle_velocity\", \"horizontal_velocity\", \"vertical_velocity\", \"angular_velocity\"\n",
    "        ]\n",
    "    else:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(STATE_DIM)]\n",
    "    \n",
    "    # Visualize SHAP summary for each action dimension.\n",
    "    for action_idx in range(ACTION_DIM):\n",
    "        print(f\"SHAP Summary Plot for Action Dimension {action_idx}\")\n",
    "        # Extract the SHAP values for the current action dimension.\n",
    "        # Expected shape: (num_explain_samples, STATE_DIM)\n",
    "        action_shap = shap_values.values[:, :, action_idx]\n",
    "        shap.summary_plot(action_shap, states_to_explain, feature_names=feature_names)\n",
    "    \n",
    "    return shap_values\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Unified SHAP Analysis for TD3 on Walker2d\")\n",
    "    parser.add_argument(\"--explainer\", type=str, default=\"unified\",\n",
    "                        choices=[\"unified\", \"kernel\", \"deep\"],\n",
    "                        help=\"Type of SHAP explainer to use (currently using unified API by default)\")\n",
    "    parser.add_argument(\"--bg\", type=str, default=\"on_policy\",\n",
    "                        choices=[\"on_policy\", \"random\"],\n",
    "                        help=\"Background data collection method\")\n",
    "    parser.add_argument(\"--num_bg\", type=int, default=200,\n",
    "                        help=\"Number of background samples\")\n",
    "    parser.add_argument(\"--num_explain\", type=int, default=50,\n",
    "                        help=\"Number of samples to explain\")\n",
    "    parser.add_argument(\"--policy_path\", type=str, default=\"Calum_Testing/ddpg_batch.pth\",\n",
    "                        help=\"Path to the policy checkpoint file\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    policy = load_policy(args.policy_path)\n",
    "    env = init_env()\n",
    "    run_shap_analysis(policy, env, bg_data_collection=args.bg, \n",
    "                      num_bg_samples=args.num_bg, num_explain_samples=args.num_explain)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
